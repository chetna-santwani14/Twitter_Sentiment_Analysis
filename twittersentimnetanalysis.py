# -*- coding: utf-8 -*-
"""TwitterSentimnetAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yiUBn9xeNUx4WhFtA8pf6hWaAiw2brAi
"""

#installing kaggle library
! pip install kaggle

# Configuring the path of kaggle.json
!mkdir .p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Importing the dataset"""

#API fetch from kaggle dataset
#!/bin/bash
!kaggle datasets download kazanova/sentiment140

#extracting the compressed dataset
from zipfile import ZipFile
dataset='/content/sentiment140.zip'


with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

"""# Why we use !


The **exclamation mark (`!`)** is not part of Linux itself ‚Äî it‚Äôs a shortcut used in **Jupyter Notebook / Google Colab** (or other IPython environments).

* Normally, you‚Äôd run commands like `mkdir`, `ls`, `cp` in a **terminal**.
* But inside a **Python notebook**, when you type code, Python is the default language.
* To run a **shell/terminal command** inside a notebook cell, you prefix it with `!`.

Example:

```python
!mkdir -p ~/.kaggle
!cp ~/Downloads/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
```

This tells the notebook: *‚ÄúDon‚Äôt treat this as Python code ‚Äî send it to the system shell instead.‚Äù*

So:

* In a **terminal/command prompt** ‚Üí you don‚Äôt use `!`.
* In a **notebook (Colab, Jupyter, Kaggle Notebooks, etc.)** ‚Üí you use `!` for shell commands.
.

# Importing the dependencies
"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

#Printing stopwords in english
print(stopwords.words('english'))

"""#What are Stopwords?

words which don't have any influentical meaning to the sentence , ml models don't require these words for processing


**Stopwords** are common words in a language that usually don‚Äôt carry much *meaning* for tasks like text classification, search, or sentiment analysis.

Examples in **English**:

```
is, am, are, the, a, an, in, on, of, at, to, for, with, and, but, or, not
```

üëâ Why remove them?
Because they appear **very frequently** in almost all texts and don‚Äôt help the model differentiate between classes. For example:

* Text 1: *‚ÄúI am happy with the product‚Äù*
* Text 2: *‚ÄúI am unhappy with the product‚Äù*

The words `I`, `am`, `with`, `the` don‚Äôt tell us much about sentiment.
But `happy` vs. `unhappy` are meaningful.

So, by removing **stopwords**, we reduce noise and focus on the **important words**.



‚ö†Ô∏è Note: Sometimes we **don‚Äôt remove stopwords** (like in chatbots or translation) because context matters.

"""

#Loading dataset using pandas
twitter_dataset=pd.read_csv('/content/training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1')
twitter_dataset.head()

twitter_dataset.shape #Here, the first row is taken as column names , so we will add the column names

columns_names=['target','id','date','flag','user','text']
twitter_dataset=pd.read_csv('/content/training.1600000.processed.noemoticon.csv',names=columns_names,encoding='ISO-8859-1')
twitter_dataset.head()

twitter_dataset.shape

twitter_dataset.isnull().sum()

#Checking the distribution for target column
twitter_dataset['target'].value_counts()

"""Converting 4 to 1

"""

twitter_dataset['target']=twitter_dataset['target'].replace(4,1)

twitter_dataset['target']

#Another way of converting 4 to 1
twitter_dataset.replace({'target':{4:1}},inplace=True)

twitter_dataset['target'].value_counts()

"""0--> means negative tweet

1--> means positive tweet

**Stemming**


---

### **Definition:**

**Stemming** is the process of **reducing a word to its root or base form**.

* The root form may not always be a real word, but it represents all forms of that word.
* This helps in **text normalization**, so that words with the same meaning are treated as the same feature.

---

### **Examples:**

| Original Word | Stemmed Word |
| ------------- | ------------ |
| playing       | play         |
| played        | play         |
| plays         | play         |
| happier       | happi        |
| happiness     | happi        |

> Notice: ‚Äúhappi‚Äù is not a real word, but it‚Äôs the root for NLP purposes.

---

### **Why we use it:**

1. Reduces the **vocabulary size** ‚Üí fewer features for ML models.
2. Helps models **understand similar words as the same concept**.
3. Commonly used in **search engines, sentiment analysis, text classification**.

---

### **In Python with NLTK:**

```python
from nltk.stem.porter import PorterStemmer

stemmer = PorterStemmer()
words = ["playing", "played", "plays", "happier", "happiness"]

stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)
```

**Output:**

```
['play', 'play', 'play', 'happi', 'happi']
```

---

‚ö†Ô∏è **Note:**

* There‚Äôs also **lemmatization**, which is similar but returns a **real word** as the root (like ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù).
* Stemming is faster but less precise than lemmatization.
"""

port_stem=PorterStemmer()

def stemming(content):
  stemmed_content=re.sub('[^a-zA-Z]',' ',content)
  stemmed_content=stemmed_content.lower()
  stemmed_content=stemmed_content.split()
  stemmed_content=[port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content=' '.join(stemmed_content)
  return stemmed_content

twitter_dataset.columns

twitter_dataset['stemmed_content']=twitter_dataset['text'].apply(stemming)

"""Nltk stands for Natural language Toolkit used for working with human language data"""

twitter_dataset.columns

twitter_dataset['stemmed_content'].head()

twitter_dataset[['text','stemmed_content']].head()

twitter_dataset['target']

#Separting the data(stemmed_content) and Label (target)
X=twitter_dataset['stemmed_content'].values
Y=twitter_dataset['target'].values

print(X)

print(Y)

"""Splitting the data into training dataset and testing dataset"""

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)

print(X.shape,X_train.shape,X_test.shape)

print(Y.shape,Y_train.shape,Y_test.shape)

#Converting the textual data into numerical form
vectorizer=TfidfVectorizer()

X_train=vectorizer.fit_transform(X_train)
X_test=vectorizer.transform(X_test)

print(X_train)

print(X_test)

"""# Training The Machine Learning Model"""

#Logistic Regression
model=LogisticRegression(max_iter=1000)

model.fit(X_train,Y_train)

"""# Model Evaluation
## Accuracy Score
"""

#Accuracy score on training data
X_train_prediction=model.predict(X_train)

Acc=accuracy_score(Y_train,X_train_prediction)

print("Accuracy Score on training data is",Acc)

#Accuracy score on testing data
X_test_prediction=model.predict(X_test)

Acc_test=accuracy_score(Y_test,X_test_prediction)

print("Accuracy Score on testing data is",Acc_test)

"""# Model Accuracy =77.6%

Saving the Trained Model
"""

import pickle

filename='training_model.sav'
pickle.dump(model,open(filename,'wb'))

"""Using the saved model for future predictions"""

#Loading the saved model
loaded_model=pickle.load(open('/content/training_model.sav','rb'))

X_new=X_test[200]
print(Y_test[200])

prediction=loaded_model.predict(X_new)
print(prediction)
if prediction[0]==0:
  print("Negative Tweet")
else :
  print("Positive Tweet")

X_new=X_test[5]
print(Y_test[5])
prediction=loaded_model.predict(X_new)
print(prediction)
if prediction[0]==0:
  print("Negative Tweet")
else :
  print("Positive Tweet")

X_new=X_test[5:12]
print(Y_test[5:12])
prediction=loaded_model.predict(X_new)
print(prediction)
["Negative" if i == 0 else "Positive" for i in prediction]

